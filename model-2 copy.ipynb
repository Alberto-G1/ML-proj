{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1740138193899,
     "user": {
      "displayName": "Katushabe annet",
      "userId": "16772122331035629141"
     },
     "user_tz": -180
    },
    "id": "WBCoVt4XpzKM"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SimpleViT.__init__() missing 6 required keyword-only arguments: 'image_size', 'patch_size', 'dim', 'depth', 'heads', and 'mlp_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Instantiate models\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     70\u001b[0m student_model \u001b[38;5;241m=\u001b[39m SemanticCNN()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Apply Sparsity\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m, in \u001b[0;36mSimpleVisionTransformer.__init__\u001b[1;34m(self, num_classes)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SimpleVisionTransformer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: SimpleViT.__init__() missing 6 required keyword-only arguments: 'image_size', 'patch_size', 'dim', 'depth', 'heads', and 'mlp_dim'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import timm\n",
    "from torch.nn.utils import prune\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from vit_pytorch.simple_vit import SimpleViT\n",
    "from termcolor import colored\n",
    "\n",
    "# Set device without printing CUDA warnings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Data with Optimized Transformations\n",
    "def get_dataloader(batch_size=16):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Reduced from 224x224\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root='C:\\\\Users\\\\user\\\\Desktop\\\\CSC YEAR 2\\\\SEM 2\\\\MACHINE LEARNING\\\\dataset3\\\\USK-Coffee\\\\train', transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root='C:\\\\Users\\\\user\\\\Desktop\\\\CSC YEAR 2\\\\SEM 2\\\\MACHINE LEARNING\\\\dataset3\\\\USK-Coffee\\\\test', transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(root='C:\\\\Users\\\\user\\\\Desktop\\\\CSC YEAR 2\\\\SEM 2\\\\MACHINE LEARNING\\\\dataset3\\\\USK-Coffee\\\\val', transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "train_loader, test_loader, val_loader = get_dataloader()\n",
    "\n",
    "# Define SimpleVisionTransformer Wrapper\n",
    "class SimpleVisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleVisionTransformer, self).__init__()\n",
    "        self.model = SimpleViT(num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define SemanticCNN Model\n",
    "class SemanticCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SemanticCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)  # Adjusted for 128x128 input size\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Instantiate models\n",
    "teacher_model = SimpleVisionTransformer().to(device)\n",
    "student_model = SemanticCNN().to(device)\n",
    "\n",
    "# Apply Sparsity\n",
    "def apply_sparsity(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "    return model\n",
    "\n",
    "apply_sparsity(student_model)\n",
    "\n",
    "# Knowledge Distillation Loss\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=3.0, alpha=0.7):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        soft_targets = nn.functional.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        teacher_targets = nn.functional.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        loss_kl = self.kl_loss(soft_targets, teacher_targets) * (self.temperature ** 2)\n",
    "        loss_ce = self.ce_loss(student_logits, labels)\n",
    "        return self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n",
    "\n",
    "# Compute Accuracy & F1 Score\n",
    "def compute_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return accuracy, f1, all_preds, all_labels\n",
    "\n",
    "# Train Model\n",
    "def train_model(teacher, student, train_loader, val_loader, test_loader, epochs=10, lr=0.001):\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "    loss_fn = DistillationLoss()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_train, total_train = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(images)\n",
    "\n",
    "            student_outputs = student(images)\n",
    "            loss = loss_fn(student_outputs, teacher_outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(student_outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        val_acc, val_f1, _, _ = compute_metrics(student, val_loader)\n",
    "\n",
    "        results.append([epoch + 1, train_acc, val_acc, val_f1, lr])\n",
    "\n",
    "    # Print results as a table\n",
    "    print(colored(\"\\nTraining Summary:\", \"cyan\", attrs=[\"bold\"]))\n",
    "    print(tabulate(results, headers=[\"Epoch\", \"Train Acc (%)\", \"Val Acc (%)\", \"Val F1-score\", \"Learning Rate\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    # Final Test Evaluation\n",
    "    test_acc, test_f1, test_preds, test_labels = compute_metrics(student, test_loader)\n",
    "    print(colored(f\"\\nFinal Test Accuracy: {test_acc:.2f}% | Final Test F1-score: {test_f1:.4f}\\n\", \"green\", attrs=[\"bold\"]))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=list(range(10)), yticklabels=list(range(10)))\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Train the student model\n",
    "train_model(teacher_model, student_model, train_loader, val_loader, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOF4kjOabkec4m1sDZgeYXA",
   "mount_file_id": "1KS_NqMjkZPxXnJZnvYQUurScd2QoouTH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "coffee_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
